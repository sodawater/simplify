from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import os
import random
import sys
import time
import logging

import numpy as np
#from six.moves import range  # pylint: disable=redefined-builtin
import tensorflow as tf

import data_utils
from generator import Generator
from discriminator import Discriminator
from rollout import Rollout
from autoencoder import Autoencoder

tf.app.flags.DEFINE_integer("total_batch", 5000, "Number of batches.")
tf.app.flags.DEFINE_integer("pretrain_epoch_gen", 50, "Pretrain epoch num for generator.")
tf.app.flags.DEFINE_integer("pretrain_epoch_dis", 50, "Pretrain epoch num for discriminator.")
tf.app.flags.DEFINE_integer("cnn_seq_len", 25, "Length of the sequence fed to CNN.")
tf.app.flags.DEFINE_integer("generated_num", 10000, "Number of samples generated by generator.")
tf.app.flags.DEFINE_float("learning_rate", 0.5, "Learning rate.")
tf.app.flags.DEFINE_float("learning_rate_decay_factor", 0.99,
                          "Learning rate decays by this much.")
tf.app.flags.DEFINE_float("max_gradient_norm", 5.0,
                          "Clip gradients to this norm.")
tf.app.flags.DEFINE_integer("batch_size", 32,
                            "Batch size to use during training.")
tf.app.flags.DEFINE_integer("size", 256, "Size of each model layer.")
tf.app.flags.DEFINE_integer("num_layers", 3, "Number of layers in the model.")
tf.app.flags.DEFINE_integer("from_vocab_size", 100000, "NormalWiki vocabulary size.")
tf.app.flags.DEFINE_integer("to_vocab_size", 30000, "SimpleWiki vocabulary size.")
tf.app.flags.DEFINE_string("data_dir", "/data/wtm/data/wikilarge/", "Data directory")
tf.app.flags.DEFINE_string("train_dir", "/data/wtm/data/wikilarge/model/GAN/", "Training directory.")
tf.app.flags.DEFINE_string("from_train_data", "/data/wtm/data/wikilarge/wiki.full.aner.ori.train.src", "Training data.")
tf.app.flags.DEFINE_string("to_train_data", "/data/wtm/data/wikilarge/wiki.full.aner.ori.train.dst", "Training data.")
tf.app.flags.DEFINE_string("from_dev_data", "/data/wtm/data/wikilarge/wiki.full.aner.ori.valid.src", "Training data.")
tf.app.flags.DEFINE_string("to_dev_data", "/data/wtm/data/wikilarge/wiki.full.aner.ori.valid.dst", "Training data.")
tf.app.flags.DEFINE_integer("max_train_data_size", 0,
                            "Limit on the size of training data (0: no limit).")
tf.app.flags.DEFINE_integer("steps_per_checkpoint", 200,
                            "How many training steps to do per checkpoint.")
tf.app.flags.DEFINE_boolean("decode", False,
                            "Set to True for interactive decoding.")
tf.app.flags.DEFINE_boolean("self_test", False,
                            "Run a self-test if this is set to True.")
tf.app.flags.DEFINE_boolean("use_fp16", False,
                            "Train using fp16 instead of fp32.")

FLAGS = tf.app.flags.FLAGS

# We use a number of buckets and pad to the closest one for efficiency.
# See seq2seq_model.Seq2SeqModel for details of how they work.
_buckets = [(10, 5), (15, 10), (25, 20), (50, 40)]

_target_buckets = [5, 10, 20, 40]
_source_buckets = [10, 15, 25, 50]

def read_data_pair(source_path, target_path, max_size=None):
  data_set = [[] for _ in _buckets]
  with tf.gfile.GFile(source_path, mode="r") as source_file:
    with tf.gfile.GFile(target_path, mode="r") as target_file:
      source, target = source_file.readline(), target_file.readline()
      counter = 0
      while source and target and (not max_size or counter < max_size):
        counter += 1
        if counter % 100000 == 0:
          print("  reading data line %d" % counter)
          sys.stdout.flush()
        source_ids = [int(x) for x in source.split()]
        target_ids = [int(x) for x in target.split()]
        target_ids.append(data_utils.EOS_ID)
        for bucket_id, (source_size, target_size) in enumerate(_buckets):
          if len(source_ids) < source_size and len(target_ids) < target_size:
            data_set[bucket_id].append([source_ids, target_ids])
            break
        source, target = source_file.readline(), target_file.readline()
  return data_set

def read_data_simple(path, max_size=None):
    data_set = [[] for _ in _target_buckets]
    with tf.gfile.GFile(path, mode="r") as target_file:
        target = target_file.readline()
        counter = 0
        while target and (not max_size or counter < max_size):
            counter += 1
            if counter % 100000 == 0:
                print("  reading data line %d" % counter)
                sys.stdout.flush()
            target_ids = [int(x) for x in target.split()]
            target_ids.append(data_utils.EOS_ID)
            for bucket_id, (target_size) in enumerate(_target_buckets):
                if len(target_ids) < target_size:
                    data_set[bucket_id].append(target_ids)
                    break
            target = target_file.readline()
    return data_set

def read_data_normal(path, max_size=None):
    data_set = [[] for _ in _source_buckets]
    with tf.gfile.GFile(path, mode="r") as source_file:
        source = source_file.readline()
        counter = 0
        while source and (not max_size or counter < max_size):
            counter += 1
            if counter % 100000 == 0:
                print("  reading data line %d" % counter)
                sys.stdout.flush()
            source_ids = [int(x) for x in source.split()]
            source_ids.append(data_utils.EOS_ID)
            for bucket_id, (source_size) in enumerate(_source_buckets):
                if len(source_ids) < source_size:
                    data_set[bucket_id].append(source_ids)
                    break
            source = source_file.readline()
    return data_set

def read_data_gen(path, max_size=None):
    data_set = [[] for _ in _target_buckets]
    with tf.gfile.GFile(path, mode="r") as target_file:
        target = target_file.readline()
        counter = 0
        while target and (not max_size or counter < max_size):
            counter += 1
            if counter % 100000 == 0:
                print("  reading data line %d" % counter)
                sys.stdout.flush()
            target_ids = [int(x) for x in target.split()]
            target_ids.append(data_utils.EOS_ID)
            for bucket_id, (target_size) in enumerate(_target_buckets):
                if len(target_ids) < target_size:
                    data_set[bucket_id].append([target_ids])
                    break
            target = target_file.readline()
    return data_set

def create_model_generator(session, forward_only):
  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32
  generator = Generator(FLAGS.to_vocab_size,
                        FLAGS.batch_size,
                        FLAGS.size,
                        FLAGS.size,
                        50,
                        data_utils.GO_ID,)
  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir + "/generator/")
  if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
      print("Reading model parameters from %s" % ckpt.model_checkpoint_path)
      generator.saver.restore(session, ckpt.model_checkpoint_path)
      return generator, False
  else:
      print("Created model with fresh parameters.")
      session.run(tf.global_variables_initializer())
      return generator, True
  return generator

def create_model_discriminator(session, forward_only):
    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32
    dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]
    dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]
    dis_dropout_keep_prob = 0.75
    dis_l2_reg_lambda = 0.2
    discriminator = Discriminator(FLAGS.batch_size,
                                  30,
                                  2,
                                  FLAGS.to_vocab_size,
                                  FLAGS.size,
                                  filter_sizes=dis_filter_sizes,
                                  num_filters=dis_num_filters,
                                  l2_reg_lambda=dis_l2_reg_lambda
                                  )
    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir + "/generator/")
    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
        print("Reading model parameters from %s" % ckpt.model_checkpoint_path)
        discriminator.saver.restore(session, ckpt.model_checkpoint_path)
        return discriminator, False
    else:
        print("Created model with fresh parameters.")
        session.run(tf.global_variables_initializer())
        return discriminator, True
    return discriminator

def create_model_autoencoder(session):
    autoencoder = Autoencoder()
    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir + "/autoencoder/")
    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
        print("Reading model parameters from %s" % ckpt.model_checkpoint_path)
        autoencoder.saver.restore(session, ckpt.model_checkpoint_path)
        return autoencoder, False
    else:
        print("Created model with fresh parameters.")
        session.run(tf.global_variables_initializer())
        return autoencoder, True
    return autoencoder

def generate_samples(sess, trainable_model, batch_size, buckets_size, buckets, output_file):
    # Generate Samples
    generated_samples = []
    for i in range(buckets):
        for _ in range(buckets_size[i] / batch_size):
            z = np.random.rand(batch_size, trainable_model.hidden_dim) * 2 - [[1] * trainable_model.hidden_dim] * batch_size
            generated_samples.extend(trainable_model.generate(sess, z, buckets[i]))

    with open(output_file, 'w') as fout:
        for poem in generated_samples:
            buffer = ' '.join([str(x) for x in poem]) + '\n'
            fout.write(buffer)

def decode_sen(outputs):
    to_vocab_path = os.path.join(FLAGS.data_dir, "vocab%d.to" % FLAGS.to_vocab_size)
    _, rev_to_vocab = data_utils.initialize_vocabulary(to_vocab_path)
    for sentence in outputs:
        sentence = sentence.tolist()
        if data_utils.EOS_ID in sentence:
            sentence = sentence[:sentence.index(data_utils.EOS_ID)]
        print(" ".join([tf.compat.as_str(rev_to_vocab[word]) for word in sentence]))

def train(from_train, to_train, from_dev, to_dev):

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session(config=config) as sess:
        print("Creating generator...")
        generator, if_new_gen = create_model_generator(sess, False)
        print("Creating discriminator...")
        discriminator, if_new_dis = create_model_discriminator(sess, False)
        dev_set = read_data_pair(from_dev, to_dev)
        train_set = read_data_pair(from_train, to_train, FLAGS.max_train_data_size)
        train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]
        train_total_size = float(sum(train_bucket_sizes))

        # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use
        # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to
        # the size if i-th training bucket, as used later.
        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size
                               for i in range(len(train_bucket_sizes))]
        train_set_sim = read_data_simple(to_train, FLAGS.max_train_data_size)
        pretrain_bucket_sizes_sim = [len(train_set_sim[b]) for b in range(len(_target_buckets))]
        pretrain_total_size_sim = float(sum(pretrain_bucket_sizes_sim))
        pretrain_buckets_scale_sim = [sum(pretrain_bucket_sizes_sim[:i + 1]) / pretrain_total_size_sim
                                      for i in range(len(pretrain_bucket_sizes_sim))]
        if if_new_gen:
            print("Pre-training generator...")
            for epoch in range(FLAGS.pretrain_epoch_gen):
                pretrain_g_loss = []
                for it in range(int(pretrain_total_size_sim / FLAGS.batch_size)):
                    random_number_01 = np.random.random_sample()
                    bucket_id = min([i for i in range(len(pretrain_buckets_scale_sim))
                                     if pretrain_buckets_scale_sim[i] > random_number_01])
                    decoder_inputs, target_weights = generator.get_batch_simple(train_set_sim, _target_buckets, bucket_id)
                    #z = np.random.rand(FLAGS.batch_size, generator.hidden_dim) * 2 - [[1] * generator.hidden_dim] * FLAGS.batch_size
                    z = np.zeros(shape=[FLAGS.batch_size, generator.hidden_dim], dtype=np.int32)
                    output_length = _target_buckets[bucket_id]
                    _, g_loss, outputs = generator.pretrain_step(sess, decoder_inputs, z, target_weights, output_length)
                    pretrain_g_loss.append(g_loss)
                    if it % 50 == 0:
                        decode_sen(outputs)
                        print(np.mean(pretrain_g_loss))
                print(np.mean(pretrain_g_loss))

        if if_new_dis:
            print("Pre-training discriminator...")
            for epoch in range(FLAGS.pretrain_epoch_dis):
                generate_samples(sess, generator, FLAGS.batch_size, pretrain_bucket_sizes_sim, _target_buckets, FLAGS.generated_file)
                train_set_gen = read_data_gen(FLAGS.generated_file, FLAGS.max_train_data_size)
                pretrain_bucket_sizes_gen = [len(train_set_gen[b]) for b in range(len(_target_buckets))]
                pretrain_total_size_gen = float(sum(pretrain_bucket_sizes_gen))
                pretrain_buckets_scale_gen = [sum(pretrain_bucket_sizes_gen[:i + 1]) / pretrain_total_size_gen
                                      for i in range(len(pretrain_bucket_sizes_gen))]
                for _ in range(int(pretrain_total_size_gen / FLAGS.batch_size)):
                    random_number_02 = np.random.random_sample()
                    bucket_id = min([i for i in range(len(pretrain_buckets_scale_gen))
                                     if pretrain_buckets_scale_gen[i] > random_number_02])
                    positive_inputs, positive_labels = discriminator.get_batch_simple(train_set_sim, _target_buckets, bucket_id, [0, 1])
                    negative_inputs, negative_labels = discriminator.get_batch_simple(train_set_gen, _target_buckets, bucket_id, [1, 0])
                    inputs = np.array(positive_inputs + negative_inputs)
                    labels = np.array(positive_labels + negative_labels)
                    shuffle_indices = np.random.permutation(np.arange(len(labels)))
                    inputs = inputs[shuffle_indices]
                    labels = labels[shuffle_indices]
                    feed1 = {discriminator.input_x:inputs[:FLAGS.cnn_seq_len],
                             discriminator.input_y:labels[:FLAGS.cnn_seq_len],
                             discriminator.dropout_keep_prob:FLAGS.dis_dropout_keep_prob}
                    feed2 = {discriminator.input_x:inputs[FLAGS.cnn_seq_len:],
                             discriminator.input_y:labels[FLAGS.cnn_seq_len:],
                             discriminator.dropout_keep_prob:FLAGS.dis_dropout_keep_prob}
                    _ = sess.run(discriminator.train_op, feed1)
                    _ = sess.run(discriminator.train_op, feed2)

        print("Adversarial training...")
        rollout = Rollout(0.8)
        for total_batch in range(FLAGS.total_batch):
            for _ in range(1):
                random_number_03 = np.random.random_sample()
                bucket_id = min([i for i in range(len(pretrain_buckets_scale_sim))
                                 if pretrain_buckets_scale_sim[i] > random_number_03])
                z = np.random.rand(FLAGS.batch_size, generator.hidden_dim) * 2 - [[1] * generator.hidden_dim] * FLAGS.batch_size
                output_length = _target_buckets[bucket_id]
                samples = generator.generate(sess, z, output_length)
                rewards = rollout.get_reward(sess, samples, 16, discriminator)
                feed = {generator.x:samples,
                        generator.reward:rewards,
                        }
                _ = sess.run(generator.g_updates, feed_dict=feed)

            rollout.update_params()

            for _ in range(300):
                random_number_04 = np.random.random_sample()
                bucket_id = min([i for i in range(len(pretrain_buckets_scale_sim))
                                 if pretrain_buckets_scale_sim[i] > random_number_04])
                z = np.random.rand(FLAGS.batch_size, generator.hidden_dim) * 2 - [[1] * generator.hidden_dim] * FLAGS.batch_size
                output_length = _target_buckets[bucket_id]
                samples = generator.generate(sess, z, output_length)
                positive_inputs, positive_labels = discriminator.get_batch_simple(train_set_sim, _target_buckets,
                                                                                  bucket_id, [0, 1])
                negative_inputs = []
                for sample in samples:
                    pad_size = FLAGS.cnn_seq_len - len(input) - 1
                    if pad_size >= 0 :
                        negative_inputs.append([data_utils.GO_ID] + sample + [data_utils.PAD_ID] * pad_size)
                    else:
                        negative_inputs.append([data_utils.GO_ID] + sample[0: FLAGS.cnn_seq_len - 1])
                negative_labels = [[1, 0] for _ in negative_inputs]
                inputs = np.array(positive_inputs + negative_inputs)
                labels = np.array(positive_labels + negative_labels)
                shuffle_indices = np.random.permutation(np.arange(len(labels)))
                inputs = inputs[shuffle_indices]
                labels = labels[shuffle_indices]
                feed1 = {discriminator.input_x: inputs[:FLAGS.cnn_seq_len],
                         discriminator.input_y: labels[:FLAGS.cnn_seq_len],
                         discriminator.dropout_keep_prob: FLAGS.dis_dropout_keep_prob}
                feed2 = {discriminator.input_x: inputs[FLAGS.cnn_seq_len:],
                         discriminator.input_y: labels[FLAGS.cnn_seq_len:],
                         discriminator.dropout_keep_prob: FLAGS.dis_dropout_keep_prob}
                _ = sess.run(discriminator.train_op, feed1)
                _ = sess.run(discriminator.train_op, feed2)

def train_ae(from_train, to_train, from_dev, to_dev):
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session(config=config) as sess:
        print("Creating auto-encoder...")
        autoencoder, if_new_ae = create_model_autoencoder(sess)
        train_set_nor = read_data_normal(from_train, FLAGS.max_train_data_size)
        train_bucket_sizes_nor = [len(train_set_nor[b]) for b in range(len(_source_buckets))]
        train_total_size_nor = float(sum(train_bucket_sizes_nor))
        train_buckets_scale_nor = [sum(train_bucket_sizes_nor[:i + 1]) / train_total_size_nor
                                      for i in range(len(train_bucket_sizes_nor))]
        if if_new_ae:
            print("Training auto-encoder...")
            for epoch in range(FLAGS.ae_epoch_num):
                train_loss = []
                for it in range(int(train_total_size_nor / FLAGS.batch_size)):
                    random_number_05 = np.random.random_sample()
                    bucket_id = min([i for i in range(len(train_buckets_scale_nor))
                                     if train_buckets_scale_nor[i] > random_number_05])



def main(_):
    from_train_data = FLAGS.from_train_data
    to_train_data = FLAGS.to_train_data
    from_dev_data = FLAGS.from_dev_data
    to_dev_data = FLAGS.to_dev_data
    from_train, to_train, from_dev, to_dev, _, _ = data_utils.prepare_data(
        FLAGS.data_dir,
        from_train_data,
        to_train_data,
        from_dev_data,
        to_dev_data,
        FLAGS.from_vocab_size,
        FLAGS.to_vocab_size)
    train_ae(from_train, to_train, from_dev, to_dev)
    train(from_train, to_train, from_dev, to_dev)


if __name__ == "__main__":
    tf.app.run()